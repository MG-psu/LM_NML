{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84918aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0ac219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26e8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/madhuri/Desktop/MasterThesis/Part1/NLM_sets_top_10_clean/NLM1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6772ce27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Leukemia patient sample                                          2096\n",
       "Breast Tumor                                                     1789\n",
       "HumanAorticEndothelialCells                                      1014\n",
       "Lymphoblastoid cell line                                          817\n",
       "pre-treatment bone marrow                                         817\n",
       "PBMC                                                              550\n",
       "Blasts and mononuclear cells, AML patient                         525\n",
       "primary colorectal adenocarcinoma                                 519\n",
       "airway epithelial cells obtained by bronchoscopy and brushing     333\n",
       "colon cancer tissue                                               331\n",
       "Name: tissue_name, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['tissue_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef54774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Blasts and mononuclear cells, AML patient': 0, 'Breast Tumor': 1, 'HumanAorticEndothelialCells': 2, 'Leukemia patient sample': 3, 'Lymphoblastoid cell line': 4, 'PBMC': 5, 'airway epithelial cells obtained by bronchoscopy and brushing': 6, 'colon cancer tissue': 7, 'pre-treatment bone marrow': 8, 'primary colorectal adenocarcinoma': 9}\n",
      "indexes: [(0, 7), (1, 9), (2, 2), (3, 4), (4, 3), (5, 5), (6, 0), (7, 6), (8, 8), (9, 1)]\n",
      "[6 6 6 ... 1 1 1]\n",
      "[6 6 6 ... 1 1 1]\n",
      "---------------------\n",
      "Hungarian method confusion matrix:\n",
      "\n",
      " [[ 522    0    0    3    0    0    0    0    0    0]\n",
      " [   3 1239    0    0    0    2    0    1    0  544]\n",
      " [   0    0 1014    0    0    0    0    0    0    0]\n",
      " [   3    0    0 1514    0    0    0  579    0    0]\n",
      " [   0    0    0    0  816    1    0    0    0    0]\n",
      " [ 512    0    0    0    0   21    0   17    0    0]\n",
      " [   0    0    0    0    0    0  333    0    0    0]\n",
      " [   0   44    0    0    0    0    0    0    0  287]\n",
      " [   0    1    0    1    0    0    0    3  812    0]\n",
      " [   0    0    0    0    0    0    0    0    0  519]]\n",
      "accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "If we use (external) classification evalutation measures like F1 or \n",
    "accuracy for clustering evaluation, problems may arise. \n",
    "\n",
    "One way to fix is to perform label matching.\n",
    "\n",
    "Here we performs kmeans clustering on the Iris dataset and proceed to use \n",
    "the Hungarian (Munkres) algorithm to correct the mismatched labeling. \n",
    "'''\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from munkres import Munkres\n",
    "\n",
    "def make_cost_matrix(c1, c2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    uc1 = np.unique(c1)\n",
    "    uc2 = np.unique(c2)\n",
    "    l1 = uc1.size\n",
    "    l2 = uc2.size\n",
    "    assert(l1 == l2 and np.all(uc1 == uc2))\n",
    "\n",
    "    m = np.ones([l1, l2])\n",
    "    for i in range(l1):\n",
    "        it_i = np.nonzero(c1 == uc1[i])[0]\n",
    "        for j in range(l2):\n",
    "            it_j = np.nonzero(c2 == uc2[j])[0]\n",
    "            m_ij = np.intersect1d(it_j, it_i)\n",
    "            m[i,j] =  -m_ij.size\n",
    "    return m\n",
    "\n",
    "def translate_clustering(clt, mapper):\n",
    "    return np.array([ mapper[i] for i in clt ])\n",
    "\n",
    "def accuracy(cm):\n",
    "    \"\"\"computes accuracy from confusion matrix\"\"\"\n",
    "    return np.trace(cm, dtype=float) / np.sum(cm)\n",
    "\n",
    "def main():\n",
    "    \"\"\"entry point\"\"\"\n",
    "    dataset = pd.read_csv('/Users/madhuri/Desktop/MasterThesis/Part1/NLM_sets_top_10_clean/NLM1.csv') # loads the dataset\n",
    "    #data, classes = dataset.data, dataset.target # data and true labels\n",
    "    unscaled_data=dataset.drop(['tissue_name'], axis=1)\n",
    "    classes_actual=dataset['tissue_name']\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    classes= le.fit_transform(classes_actual)\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(le_name_mapping)\n",
    "    \n",
    "    #Standardizing\n",
    "    sc=StandardScaler()\n",
    "    data = sc.fit_transform(unscaled_data)\n",
    "    \n",
    "    algo = KMeans(n_clusters=10, random_state = 0)\n",
    "\n",
    "    labels = algo.fit(data).labels_ # performs the algo and get the predicted labels\n",
    "    #attaching labels to the datset\n",
    "    dataset['cluster_label']=labels\n",
    "    #print(dataset.head(2))\n",
    "    \n",
    "    num_labels = len(np.unique(classes))\n",
    "\n",
    "    cm = confusion_matrix(classes, labels, labels=range(num_labels)) # gets the confusion matrix\n",
    "    #print (\"---------------------\\nold confusion matrix:\\n\" \\\n",
    "     #     \" %s\\naccuracy: %.2f\" % (str(cm), accuracy(cm)))\n",
    "\n",
    "    cost_matrix = make_cost_matrix(labels, classes)\n",
    "\n",
    "    m = Munkres()\n",
    "    indexes = m.compute(cost_matrix)\n",
    "    print(\"indexes:\", indexes)\n",
    "    mapper = { old: new for (old, new) in indexes }\n",
    "\n",
    "    #print (\"---------------------\\nmapping:\")\n",
    "    #for old, new in mapper['iteritems']:\n",
    "    #for old, new in mapper.iteritems():\n",
    "        \n",
    "    #print (\"map: %s --> %s\" %(old, new))\n",
    "\n",
    "    new_labels = translate_clustering(labels, mapper)\n",
    "    print(classes[:-200])\n",
    "    print(new_labels[:-200])\n",
    "   \n",
    "    new_cm = confusion_matrix(classes, new_labels, labels=range(num_labels))\n",
    "    print (\"---------------------\\nHungarian method confusion matrix:\\n\\n\" \\\n",
    "          \" %s\\naccuracy: %.2f\" % (str(new_cm), accuracy(new_cm)))\n",
    "   \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "#main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019a4ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def kmeans_assignment(centroids, points):\n",
    "    num_centroids, dim = centroids.shape\n",
    "    num_points, _ = points.shape\n",
    "\n",
    "    # Tile and reshape both arrays into `[num_points, num_centroids, dim]`.                                                                      \n",
    "    centroids = np.tile(centroids, [num_points, 1]).reshape([num_points, num_centroids, dim])\n",
    "    points = np.tile(points, [1, num_centroids]).reshape([num_points, num_centroids, dim])\n",
    "\n",
    "    # Compute all distances (for all points and all centroids) at once and                                                                       \n",
    "    # select the min centroid for each point.                                                                                                    \n",
    "    distances = np.sum(np.square(centroids - points), axis=2)\n",
    "    return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe85f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
